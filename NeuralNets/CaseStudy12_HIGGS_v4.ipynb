{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Case Study 12: Neural Networks</center></h1>\n",
    "<h3><center>Steven Cocke, Hannah Kosinovsky, Tanvi Arora</center></h3>\n",
    "<h3><center>November 18th, 2019</center></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Abstract</center></h3>\n",
    "\n",
    "In this case study we attempted to replicate the work in the paper on \"Searching for Exotic Particles in High-Energy Physics with Deep Learning\" by adjusting the parameters and hyper parameters in keras to build a deep learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Introduction\n",
    "\n",
    "Neural Networks are statistical models that were named after brain neurons. Neurons in the brain fires after a certain threshold is reached. In the statistical model, after the inputs of the model reach a certain threshold, the neuron in the model also \"fires\". Since we have multiple neurons in the model, it becomes a network. Each neuron in our model computes an \"activation function\".\n",
    "\n",
    "These activation functions act as logistic regressors. Since the layers in the network perform these functions several times over on each previous output, the neural networks are essentially doing regressions on top of one another. \n",
    "\n",
    "The goal is to change the inputs such that the model has more complex indpendent components. Each neuron in the network has its own weight and bias. \n",
    "\n",
    "In the paper from which we are trying to replicate results, *I need help describing the performance results from the paper itself, not sure which values are most relevent I'm sorry*\n",
    "\n",
    "By following the methodology for the inputs, we should be able to obtain similar accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Methods\n",
    "\n",
    "In order to replicate these results, we had to focus on several different inputs. In the paper it was specified that they used \n",
    "\n",
    "- tanh activation functions in the hidden layers\n",
    "\n",
    "<a id='tanhmethods'></a>\n",
    "<a href=\"#tanh\">Tanh Activation Function</a>\n",
    "\n",
    "- weights initialized from a normal distribution with zero mean and standard deviation of .1. \n",
    "\n",
    "<a id='weightmethods'></a>\n",
    "<a href=\"#weights\">Weight Initialization</a>\n",
    "\n",
    "- gradient computations made on mini-batches of size 100\n",
    "\n",
    "<a id='minibatchmethods'></a>\n",
    "<a href=\"#minibatch\">Mini-Batches</a>\n",
    "\n",
    "- momentum that increases linearly between .9 and .99\n",
    "\n",
    "<a id='momentummethods'></a>\n",
    "<a href=\"#momentum\">Momentum Adjusting</a>\n",
    "\n",
    "- Implementation of early stopping after a set threshold to prevent overfitting\n",
    "\n",
    "<a id='stoppingmethods'></a>\n",
    "<a href=\"#stopping\">Early Stopping</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Results\n",
    "\n",
    "*compare our final results to the accuracy rate that they had. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Conclusions\n",
    "\n",
    "We were not able to obtain an accuracy rate as high as the one in the paper. It is possible that there were additional inputs not accurately specified. \n",
    "\n",
    "Overall it is no longer good practice to use Stochastic Gradient Descent (SGD) when using tensorflow to produce neural networks. Now, it has been shown that the optimizer \"Adam\" outperforms SGD, which is shown to be better for shallow networks. In order to improve future results we would suggest using Adam as the optimizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Paper on Deep Neural Networks used to replicate results: \n",
    "https://arxiv.org/pdf/1402.4735.pdf\n",
    "\n",
    "Comparison of keras optimizers:\n",
    "https://www.dlology.com/blog/quick-notes-on-how-to-choose-optimizer-in-keras/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers \n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(tf.__version__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs=pd.read_csv('HIGGS.csv',sep=',',nrows=2.6E6,header=None,dtype=np.float16)\n",
    "#higgs=pd.read_csv('HIGGS.csv',sep=',',header=None,dtype=np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs.columns=['class_label','lepton_pT','lepton_eta','lepton_phi','missing_energy_magnitude','missing_energy_phi','jet_1_pt','jet_1_eta','jet_1_phi','jet_1_b_tag','jet_2_pt','jet_2_eta','jet_2_phi','jet_2_b_tag','jet_3_pt','jet_3_eta','jet_3_phi','jet_3_b_tag','jet_4_pt','jet_4_eta','jet_4_phi','jet_4_b_tag','m_jj','m_jjj','m_lv','m_jlv','m_bb','m_wbb','m_wwbb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higgs.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=higgs.copy()\n",
    "x=x.drop(['class_label'],axis=1)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=higgs['class_label']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_train = scaler.fit_transform(x)\n",
    "\n",
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "print(\"Note: median values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[12], scaler.min_[12]))\n",
    "multiplied_by = scaler.scale_[12]\n",
    "added = scaler.min_[12]\n",
    "\n",
    "scaled_train_df = pd.DataFrame(scaled_train, columns=x.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in scaled_train_df:\n",
    "    scaled_train_df[i].hist()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tanh'></a>\n",
    "<a href=\"#tanhmethods\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Dense(300, activation='tanh'))\n",
    "model.add(layers.Dense(300, activation='tanh'))\n",
    "model.add(layers.Dense(300, activation='tanh'))\n",
    "model.add(layers.Dense(300, activation='tanh'))\n",
    "model.add(layers.Dense(1,activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "learning_rate=0.05\n",
    "decay_rate=1E-5\n",
    "sgd = SGD(lr=learning_rate,  decay=decay_rate, nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "#model.compile(optimizer='sgd',loss='mean_squared_error',metrics=['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(x))\n",
    "print(type(y))\n",
    "\n",
    "y= np.asarray(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit(scaled_train_df.values, y, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_train_df, y, test_size=0.38, random_state=1776)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "print(type(np.asarray(y_test)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test data using `evaluate`\n",
    "print('\\n# Evaluate on test data')\n",
    "results = model.evaluate(np.asarray(x_test), np.asarray(y_test), batch_size=128)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions (probabilities -- the output of the last layer)\n",
    "# on new data using `predict`\n",
    "#print('\\n# Generate predictions for 3 samples')\n",
    "predictions = model.predict(np.asarray(x))\n",
    "print('predictions shape:', predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "fpr_keras, tpr_keras, thresholds_keras = metrics.roc_curve(y,predictions,pos_label=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hidden layers - tanh function  \n",
    "\n",
    "initial weights - normal distribution with zero mean and std = 0.1 in first layer , 0.001 in the output layer and 0.05 in all other hidden layers\n",
    "\n",
    "gradient computations on batch of size = 100\n",
    "\n",
    "\n",
    "momentum - increased linearly over first 200 epochs from 0.9 to 0.99 , after which it is constant  \n",
    "\n",
    "learning rate decayed by a factor of 1.0000002 every batch update until it reached a minimum of 10 power -6\n",
    "\n",
    "Training ended when momentum reached maximum value and min error on validation set had not decreased by more than a factor of 0.00001 over 10 epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='weights'></a>\n",
    "<a href=\"#weightmethods\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_w1=model.layers[1].get_weights()\n",
    "print(o_w1[0].shape)\n",
    "print(o_w1[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(np.random.normal(0, 0.1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w1= [np.random.normal(0, 0.1, 300),np.asarray([0]*300)]\n",
    "model.layers[1].set_weights(w1)\n",
    "#print(type(w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1= [np.random.normal(0, 0.1, 300),[0]*300]\n",
    "model.layers[1].set_weights(w1)\n",
    "w2= np.random.normal(0, 0.05, 300)\n",
    "model.layers[2].set_weights(w2)\n",
    "w3= np.random.normal(0, 0.05, 300)\n",
    "model.layers[3].set_weights(w3)\n",
    "w4= np.random.normal(0, 0.05, 300)\n",
    "model.layers[4].set_weights(w4)\n",
    "w5= np.random.normal(0, 0.001, 1)\n",
    "model.layers[5].set_weights(w5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='minibatch'></a>\n",
    "<a href=\"#minibatchmethods\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.05\n",
    "    lrate=initial_lrate/((1+epoch)*1.0000002)\n",
    "    if lrate>=10E-6:\n",
    "        return lrate\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='stopping'></a>\n",
    "<a href=\"#stoppingmethods\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "earlystopper= EarlyStopping(monitor='val_loss',min_delta=0.00001,patience=10)\n",
    "callbacks_list = [lrate,earlystopper]\n",
    "# Fit the model\n",
    "model.fit(scaled_train_df.values, y, validation_split=0.3, epochs=40, batch_size=100, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='momentum'></a>\n",
    "<a href=\"#momentummethods\">Back to Top</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_momentum(epoch):\n",
    "    return min(epoch*(0.09/200),0.09)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model.predict(np.asarray(x))\n",
    "fpr_keras2, tpr_keras2, thresholds_keras2 = metrics.roc_curve(y,predictions2,pos_label=None)\n",
    "auc_keras2 = auc(fpr_keras2, tpr_keras2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], 'k--')\n",
    "plt.plot(fpr_keras2, tpr_keras2, label='Keras (area = {:.3f})'.format(auc_keras2))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=learning_rate,  decay=decay_rate,nesterov=False)\n",
    "model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.Sequential()\n",
    "model2.add(layers.Dense(300, activation='tanh',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.1, seed=None)))\n",
    "model2.add(layers.Dense(300, activation='tanh',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "model2.add(layers.Dense(300, activation='tanh',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "model2.add(layers.Dense(300, activation='tanh',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=None)))\n",
    "model2.add(layers.Dense(1,activation='tanh',kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.001, seed=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate=0.05\n",
    "decay_rate=1E-5\n",
    "sgd = SGD(lr=learning_rate,  decay=decay_rate, momentum=0.09,nesterov=False)\n",
    "model2.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.fit(scaled_train_df.values, y, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning schedule callback\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "earlystopper= EarlyStopping(monitor='val_loss',min_delta=0.00001,patience=10)\n",
    "callbacks_list = [lrate,earlystopper]\n",
    "# Fit the model\n",
    "model2.fit(scaled_train_df.values, y, validation_split=0.3, epochs=40, batch_size=100, callbacks=callbacks_list, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(np.asarray(scaled_train_df.values))\n",
    "fpr_keras2, tpr_keras2, thresholds_keras2 = metrics.roc_curve(y,predictions2,pos_label=None)\n",
    "auc_keras2 = auc(fpr_keras2, tpr_keras2)\n",
    "print(auc_keras2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], 'k--')\n",
    "plt.plot(fpr_keras2, tpr_keras2, label='Keras (area = {:.3f})'.format(auc_keras2))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
